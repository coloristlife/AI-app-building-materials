# tools

- https://github.com/huggingface/smolagents/tree/main/examples/open_deep_research
    - https://huggingface.co/blog/open-deep-research

  Deep Research is composed of an LLM (such as 4o, o1, or o3) and an internal "agentic framework" that guides the LLM to use tools (like web search) and organize its actions in steps.
  
  ## Building an open Deep Research
  
  - **Using a CodeAgent**
  
  The first improvement over traditional AI agent systems we’ll tackle is to use a so-called “code agent”. As shown by Wang et al. (2024), letting the agent express its actions in code has several advantages, but most notably that code is specifically designed to express complex sequences of actions.

  <img width="2300" height="968" alt="image" src="https://github.com/user-attachments/assets/c3743d2e-6210-4616-ab26-c52efc36f55a" />

  Advantages of using code actions include:
  
      ◦ They are much more concise than JSON.
  
      ◦ Code actions require 30% fewer steps than JSON on average, resulting in an equivalent reduction in generated tokens, making agent system runs ~30% cheaper.
  
      ◦ Code allows for the reuse of tools from common libraries.
  
      ◦ Code offers better performance in benchmarks due to a more intuitive way to express actions and LLMs' extensive exposure to code during training.
  
      ◦ Code provides a better handling of state, which is particularly useful for multimodal tasks (e.g., storing an image as a variable for later use).
  
  
    smolagents: a barebones library for agents that think in code.
    - https://github.com/huggingface/smolagents/tree/main
 
  -  **Making the right tools**
 
    1. A web browser. While a fully fledged web browser interaction like Operator will be needed to reach full performance, we started with an extremely simple text-based web browser for now for our first proof-of-concept. You can find the code [here](https://github.com/huggingface/smolagents/blob/main/examples/open_deep_research/scripts/text_web_browser.py)

  2. A simple text inspector, to be able to read a bunch of text file format, find it [here](https://github.com/huggingface/smolagents/blob/main/examples/open_deep_research/scripts/text_web_browser.py).
 
     These tools were taken from the excellent [Magentic-One](https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/) agent by Microsoft Research.

- https://github.com/jina-ai/node-DeepResearch
  
  Keep searching, reading webpages, reasoning until it finds the answer (or exceeding the token budget).
  
  We use Gemini (latest gemini-2.0-flash) / OpenAI / LocalLLM for reasoning, Jina Reader for searching and reading webpages, you can get a free API key with 1M tokens from jina.ai.

  https://search.jina.ai/
  
  
- https://github.com/mshumer/OpenDeepResearcher
  
  This notebook implements an AI researcher that continuously searches for information based on a user query until the system is confident that it has gathered all the necessary details. It makes use of several services to do so:

  SERPAPI: To perform Google searches.
  Jina: To fetch and extract webpage content.
  OpenRouter (default model: anthropic/claude-3.5-haiku): To interact with a LLM for generating search queries, evaluating page relevance, and extracting context.
  
- https://github.com/assafelovic/gpt-researcher
  
  An LLM agent that conducts deep research (local and web) on any given topic and generates a long report with citations.

- https://github.com/nickscamara/open-deep-research
  
  An open source deep research clone. AI Agent that reasons large amounts of web data extracted with Firecrawl


# The core of deep research 

- A Practical Guide to Implementing DeepSearch/DeepResearch
  
  https://jina.ai/news/a-practical-guide-to-implementing-deepsearch-deepresearch/

  DeepResearch builds upon DeepSearch by adding a structured framework for generating long research reports. It often begins by creating a table of contents, then systematically applies DeepSearch to each required section – from introduction through related work and methodology, all the way to the conclusion.

  Each section is generated by feeding specific research questions into the DeepSearch. The final phase involves consolidating all sections into a single prompt to improve the overall narrative coherence.

  <img width="2268" height="944" alt="image" src="https://github.com/user-attachments/assets/58ca0c9a-bb15-421a-971a-a94469ef0fd7" />

    The heart of DeepSearch lies in its loop reasoning approach. Rather than attempting to answer questions in a single-pass like most RAG systems, We have implemented an iterative loop that continuously searches for information, reads relevant sources, and reasons until it finds an answer or exhausts the token budget. Here's the simplified core of this big while loop:

~~~~
  // Main reasoning loop
while (tokenUsage < tokenBudget && badAttempts <= maxBadAttempts) {
  // Track progression
  step++; totalStep++;
  
  // Get current question from gaps queue or use original question
  const currentQuestion = gaps.length > 0 ? gaps.shift() : question;
  
  // Generate prompt with current context and allowed actions
  system = getPrompt(diaryContext, allQuestions, allKeywords, 
                    allowReflect, allowAnswer, allowRead, allowSearch, allowCoding,
                    badContext, allKnowledge, unvisitedURLs);
  
  // Get LLM to decide next action
  const result = await LLM.generateStructuredResponse(system, messages, schema);
  thisStep = result.object;
  
  // Execute the selected action (answer, reflect, search, visit, coding)
  if (thisStep.action === 'answer') {
    // Process answer action...
  } else if (thisStep.action === 'reflect') {
    // Process reflect action...
  } // ... and so on for other actions
}

~~~~
A key implementation detail is selectively disabling certain actions at each step to ensure more stable structured output. For example, if there are no URLs in memory, we disable the visit action; or if the last answer was rejected, we prevent the agent from immediately calling answer again. This constraint keeps the agent on a productive path, avoiding repetitive failures caused by invoking same action.
  
**System Prompt**

We use XML tags to define sections, which produces more robust system prompt and generations. we also found that placing field constraints directly inside JSON schema description fields yields better results. While some might argue that most prompts could be automated with reasoning models like DeepSeek-R1, the context length restrictions and the need for highly specific behavior make an explicit approach more reliable in practice.

~~~~
function getPrompt(params...) {
  const sections = [];
  
  // Add header with system instruction
  sections.push("You are an advanced AI research agent specialized in multistep reasoning...");
  
  // Add accumulated knowledge section if exists
  if (knowledge?.length) {
    sections.push("<knowledge>[Knowledge items]</knowledge>");
  }
  
  // Add context of previous actions
  if (context?.length) {
    sections.push("<context>[Action history]</context>");
  }
  
  // Add failed attempts and learned strategies
  if (badContext?.length) {
    sections.push("<bad-attempts>[Failed attempts]</bad-attempts>");
    sections.push("<learned-strategy>[Improvement strategies]</learned-strategy>");
  }
  
  // Define available actions based on current state
  sections.push("<actions>[Available action definitions]</actions>");
  
  // Add response format instruction
  sections.push("Respond in valid JSON format matching exact JSON schema.");
  
  return sections.join("\n\n");
}
~~~~

**Gap Questions Traversing**

In DeepSearch, "gap questions" represent knowledge gaps that need to be filled before answering the main question. Rather than directly tackling the original question, the agent identifies sub-questions that will build the necessary knowledge foundation.

This approach creates a FIFO (First-In-First-Out) queue with rotation, where:

- New gap questions are pushed to the front of the queue
- The original question is always pushed to the back
- The system pulls from the front of the queue at each step

What makes this design great is that it maintains a single shared context across all questions. When a gap question is answered, that knowledge becomes immediately available for all subsequent questions, including when we eventually revisit the original question.

**FIFO Queue vs Recursion**

An alternative approach is using recursion, which corresponds to depth-first search. Each gap question spawns a new recursive call with its own isolated context. The system must completely resolve each gap question (and all of its potential sub-questions) before returning to the parent question.


In the recursive approach, the system would have to fully resolve Q1 (potentially generating its own sub-questions) after every gap question and their sub-questions! This is a big contrast to the queue approach, which processes questions where Q1 gets revisited right after 3 gap questions.


In reality, we found the recursion approach is very hard to apply budget-forcing to, since there is no clear rule of thumb for how much token budget we should grant for sub-questions (since they may spawn new sub-questions). The benefit from clear context separation in the recursion approach is very marginal compared to the complicated budget forcing and late return problems. This FIFO queue design balances depth and breadth, ensuring the system always returns to the original question with progressively better knowledge, rather than getting lost in a potentially infinite recursive descent.


**Query Rewrite**

An interesting challenge we encountered was rewriting search queries effectively:

~~~~
// Within search action handler
if (thisStep.action === 'search') {
  // Deduplicate search requests
  const uniqueRequests = await dedupQueries(thisStep.searchRequests, existingQueries);
  
  // Rewrite natural language queries into more effective search queries
  const optimizedQueries = await rewriteQuery(uniqueRequests);
  
  // Ensure we don't repeat previous searches
  const newQueries = await dedupQueries(optimizedQueries, allKeywords);
  
  // Execute searches and store results
  for (const query of newQueries) {
    const results = await searchEngine(query);
    if (results.length > 0) {
      storeResults(results);
      allKeywords.push(query);
    }
  }
}
~~~~

Query rewriting turned out to be surprisingly important - perhaps one of the most critical elements that directly determines result quality. A good query rewriter doesn't just transform natural language to BM25-like keywords; it expands queries to cover more potential answers across different languages, tones, and content formats.

For query deduplication, we initially used an LLM-based solution, but found it difficult to control the similarity threshold. We eventually switched to [jina-embeddings-v3](https://jina.ai/api-dashboard/embedding), which excels at semantic textual similarity tasks. This enables cross-lingual deduplication without worrying that non-English queries would be filtered. The embedding model ended up being crucial not for memory retrieval as initially expected, but for efficient deduplication.

**Crawling Web Content**

Web scraping and content processing is another critical component. Here we use Jina Reader API. Note that besides full webpage content, we also aggregate all snippets returned from the search engine as extra knowledge for the agent to later conclude on. Think of them as soundbites.

~~~
// Visit action handler
async function handleVisitAction(URLs) {
  // Normalize URLs and filter out already visited ones
  const uniqueURLs = normalizeAndFilterURLs(URLs);
  
  // Process each URL in parallel
  const results = await Promise.all(uniqueURLs.map(async url => {
    try {
      // Fetch and extract content
      const content = await readUrl(url);
      
      // Store as knowledge
      addToKnowledge(`What is in ${url}?`, content, [url], 'url');
      
      return {url, success: true};
    } catch (error) {
      return {url, success: false};
    } finally {
      visitedURLs.push(url);
    }
  }));
  
  // Update diary based on success or failure
  updateDiaryWithVisitResults(results);
}

~~~

**Ranking URLs for Next Read**

During a DeepSearch session, you'll likely collect a lot of URLs from search engine results and discover even more every time you read individual webpages. The total count of unique URLs can easily reach the thousands. It's crucial to guide the LLM toward URLs that have the highest probability of containing the answer you need.

We rank URLs based on multiple factors to identify the most promising ones to visit next. The algorithm combines frequency, domain, path structure, and semantic relevance to create a composite score:

1. Frequency Signals: URLs that appear multiple times across different sources receive additional weight. URLs from domains that appear frequently in search results receive a boost, as popular domains often contain authoritative content.
   
2. Path Structure: We analyze URL paths to identify content clusters. URLs within common path hierarchies receive higher scores, with a decay factor applied to deeper paths.
   
3. Semantic Relevance: For question-based searches, we use Jina Reranker to assess the semantic match between the question and the textual info of each URL, which is[ a classic reranking problem](https://jina.ai/reranker/#what_reranker). The textual info of each URL comes from:
   
    1. Title & snippets from SERP API results (https://s.jina.ai/ with 'X-Respond-With': 'no-content')
       Header 'X-Respond-With': 'no-content':
        This tells the API not to include the full webpage text, only the metadata (title + snippet).
       
    2. Anchor text of on-page URLs (https://r.jina.ai with 'X-With-Links-Summary': 'all')
        Header 'X-With-Links-Summary': 'all':
       
        This instructs the API to:
        
        - Include all URLs found on the page, and
        
        - Include a summary or context for each link.

**Memory Management**

A key challenge in multi-step reasoning is managing the agent memory effectively. We've designed the memory system to differentiate between what counts as "memory" versus what counts as "knowledge". Either way, they are all part of the LLM prompt context, separated with different XML tags:

~~~~
// Add knowledge item to accumulated knowledge
function addToKnowledge(question, answer, references, type) {
  allKnowledge.push({
    question: question,
    answer: answer,
    references: references,
    type: type,  // 'qa', 'url', 'coding', 'side-info'
    updated: new Date().toISOString()
  });
}

// Record step in narrative diary
function addToDiary(step, action, question, result, evaluation) {
  diaryContext.push(`
At step ${step}, you took **${action}** action for question: "${question}"
[Details of what was done and results]
[Evaluation if applicable]
`);
}
~~~~
Since most 2025 LLMs have substantial context windows, we opted not to use vector databases. Instead, memory consists of acquired knowledge, visited sites, and records of failed attempts - all maintained in the context. This comprehensive memory system gives the agent awareness of what it knows, what it's tried, and what's worked or failed.

**Answer Evaluation**

One key insight is that answer generation and evaluation should not be in the same prompt. In our implementation, we first determine which evaluation criteria to use when a new question arrives, and then evaluate each criterion one by one. The evaluator uses few-shot examples for consistent assessment, ensuring higher reliability than self-evaluation.

~~~~
// Separate evaluation phase
async function evaluateAnswer(question, answer, metrics, context) {
  // First, identify evaluation criteria based on question type
  const evaluationCriteria = await determineEvaluationCriteria(question);
  
  // Then evaluate each criterion separately
  const results = [];
  for (const criterion of evaluationCriteria) {
    const result = await evaluateSingleCriterion(criterion, question, answer, context);
    results.push(result);
  }
  
  // Determine if answer passes overall evaluation
  return {
    pass: results.every(r => r.pass),
    think: results.map(r => r.reasoning).join('\n')
  };
}

~~~~

**More to read:**
- https://jina.ai/news/snippet-selection-and-url-ranking-in-deepsearch-deepresearch/
