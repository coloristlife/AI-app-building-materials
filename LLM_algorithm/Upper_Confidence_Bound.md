# from AI

---

### ğŸ¯ 1. UCB Bandits (Upper Confidence Bound)

This comes from the **multi-armed bandit problem** â€” imagine youâ€™re at a casino with many slot machines.
Each machine gives random rewards, and you want to find the best one without wasting time.

The UCB strategy says:

> â€œTry each machine a bit, then keep choosing the one with the **best average reward + some uncertainty margin**.â€

That â€œmarginâ€ (the **upper confidence bound**) makes you occasionally explore machines youâ€™re less sure about â€” so you donâ€™t get stuck on a â€œpretty goodâ€ one too early.

ğŸ§  In prompt tuning:
Each **prompt candidate** is like a slot machine.
The â€œrewardâ€ is how well the model performs (say, accuracy or score).
UCB helps you test a few prompts and then focus on the most promising ones.

---

### ğŸ” 2. Successive Rejects

This is another bandit strategy, but simpler.
Instead of adding uncertainty bonuses, it works in **rounds**:

1. Test all prompts for a while.
2. Eliminate the **worst-performing** one.
3. Test the rest more deeply.
4. Repeat until one best prompt remains.

So itâ€™s a **gradual elimination** method.

ğŸ§  In prompt tuning:
You start with many edited prompts â†’ evaluate â†’ drop the weakest â†’ keep refining the survivors.

---

### ğŸ” Summary:

| Method                 | Core Idea                                            | How It Helps Prompt Search          |
| ---------------------- | ---------------------------------------------------- | ----------------------------------- |
| **UCB Bandits**        | Balances trying new options vs. exploiting good ones | Avoids missing better prompts       |
| **Successive Rejects** | Eliminates the worst options step by step            | Efficiently narrows down candidates |

---


Letâ€™s build UCB bandits up step by step ğŸ‘‡

---

### ğŸ° Imagine this:

Youâ€™re testing **3 slot machines** (or prompts).
Each time you try one, you get a *reward* (maybe a performance score).
At first, you donâ€™t know which one is best â€” so you have to **explore**.
But after some tries, you want to **exploit** the one that looks best.

This is called the **exploration vs. exploitation tradeoff.**

---

### ğŸ“ˆ The Upper Confidence Bound (UCB)

UCB gives you a simple formula to balance the two:
`UCB value=average reward+uncertainty term`

* The **average reward** = how well this option has done so far.
* The **uncertainty term** = how little youâ€™ve tried it (more uncertainty = bigger bonus).

So:

* A well-performing prompt â†’ high average reward.
* A rarely tried prompt â†’ large uncertainty bonus.

You always pick the prompt with the **highest UCB value**.
That way, you sometimes explore uncertain options â€” but mostly focus on those that perform well.

---

### ğŸ’¡ Example

| Prompt | Average Score | Times Tested | UCB Value (approx)       | Decision       |
| ------ | ------------- | ------------ | ------------------------ | -------------- |
| A      | 0.8           | 10           | 0.8 + small bonus â†’ 0.82 | maybe good     |
| B      | 0.7           | 2            | 0.7 + big bonus â†’ 0.9    | try this again |
| C      | 0.6           | 20           | 0.6 + tiny bonus â†’ 0.61  | skip it        |

So even though **A** looks best by average, **B** gets picked because itâ€™s still uncertain â€” it *might* turn out better.

---



