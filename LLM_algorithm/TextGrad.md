# NEW TextGrad by Stanford: Better than DSPy
- https://www.youtube.com/watch?v=Qks4UEsRwl0

  <img width="1189" height="649" alt="image" src="https://github.com/user-attachments/assets/952f5c3b-c1e2-484c-8125-ab8e1678d482" />


 <img width="1186" height="620" alt="image" src="https://github.com/user-attachments/assets/2b3b37b8-27f8-4875-95e5-17f910c8457c" />
**ProTeGi** is about improving **prompt engineering** using **textual (or natural language) gradients**.

Let‚Äôs unpack it *step by step* so it‚Äôs clear.

---

### üß© 1. Natural Language Gradients

Think of these like **gradients in regular machine learning**, but instead of numbers, they happen in **language space**.

* In normal ML, a *gradient* tells you which direction to move to improve a model (e.g., lower loss).
* Here, a *natural language gradient* tells you how to improve a **prompt** ‚Äî it points out *what‚Äôs wrong* with the current text.

üëâ Example:
If your prompt is ‚ÄúSummarize this text briefly,‚Äù but the model keeps giving long answers,
the *natural language gradient* might say:

> ‚ÄúThe prompt should emphasize conciseness more strongly.‚Äù

So it‚Äôs like feedback ‚Äî the LLM critiques your prompt.

---

### ‚úèÔ∏è 2. Prompt Editing

Once you have those ‚Äúgradients‚Äù (textual feedback), you edit the prompt to fix the issues.

The model uses another instruction to rewrite the prompt *in the opposite direction* of the problems.
In our example, it might change the prompt to:

> ‚ÄúSummarize this text in **one or two short sentences**.‚Äù

That‚Äôs moving ‚Äúsemantically opposite‚Äù to the problem the gradient described.

---

### üîç 3. Beam Search and Bandit Selection

Now, when improving prompts, there are many possible edits ‚Äî too many to test them all.
So these algorithms help you **search efficiently** for the best one:

* **Beam search** ‚Üí keeps a small set of the best prompt candidates as it explores.
* **Bandit selection** ‚Üí tries different options and uses feedback (like success rates) to pick the best-performing prompts efficiently.

So together, they help the system *explore many prompt variations without wasting compute.*

---
Beam Search and Bandit Selection is a key part of how the system *chooses* which prompts to keep.

Let‚Äôs unpack both ideas simply, then connect them to prompt optimization.

---

### üéØ 1. UCB Bandits (Upper Confidence Bound)

This comes from the **multi-armed bandit problem** ‚Äî imagine you‚Äôre at a casino with many slot machines.
Each machine gives random rewards, and you want to find the best one without wasting time.

The UCB strategy says:

> ‚ÄúTry each machine a bit, then keep choosing the one with the **best average reward + some uncertainty margin**.‚Äù

That ‚Äúmargin‚Äù (the **upper confidence bound**) makes you occasionally explore machines you‚Äôre less sure about ‚Äî so you don‚Äôt get stuck on a ‚Äúpretty good‚Äù one too early.

üß† In prompt tuning:
Each **prompt candidate** is like a slot machine.
The ‚Äúreward‚Äù is how well the model performs (say, accuracy or score).
UCB helps you test a few prompts and then focus on the most promising ones.

---

### üîÅ 2. Successive Rejects

This is another bandit strategy, but simpler.
Instead of adding uncertainty bonuses, it works in **rounds**:

1. Test all prompts for a while.
2. Eliminate the **worst-performing** one.
3. Test the rest more deeply.
4. Repeat until one best prompt remains.

So it‚Äôs a **gradual elimination** method.

üß† In prompt tuning:
You start with many edited prompts ‚Üí evaluate ‚Üí drop the weakest ‚Üí keep refining the survivors.

---

### üîç Summary:

| Method                 | Core Idea                                            | How It Helps Prompt Search          |
| ---------------------- | ---------------------------------------------------- | ----------------------------------- |
| **UCB Bandits**        | Balances trying new options vs. exploiting good ones | Avoids missing better prompts       |
| **Successive Rejects** | Eliminates the worst options step by step            | Efficiently narrows down candidates |

---



# Repo
- https://github.com/zou-group/textgrad

<img width="3725" height="1030" alt="image" src="https://github.com/user-attachments/assets/6ff0d694-61a1-494a-8060-2590ca31eedb" />

TextGrad: Automatic ''Differentiation'' via Text

An autograd engine -- for textual gradients!

TextGrad is a powerful framework building automatic ``differentiation'' via text. TextGrad implements backpropagation through text feedback provided by LLMs, strongly building on the gradient metaphor

We provide a simple and intuitive API that allows you to define your own loss functions and optimize them using text feedback. This API is similar to the Pytorch API, making it simple to adapt to your usecases.


- code analysis:

  - https://zread.ai/zou-group/textgrad/9-automatic-differentiation-for-text
  
  TextGrad revolutionizes the way we optimize text-based systems by implementing automatic "differentiation" through text feedback. This powerful framework extends the familiar concept of gradient-based optimization from numerical domains to the world of text, enabling us to optimize prompts, solutions, and other text-based variables using large language models (LLMs) as gradient engines.


  
