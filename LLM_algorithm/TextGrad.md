# NEW TextGrad by Stanford: Better than DSPy
- https://www.youtube.com/watch?v=Qks4UEsRwl0

  <img width="1189" height="649" alt="image" src="https://github.com/user-attachments/assets/952f5c3b-c1e2-484c-8125-ab8e1678d482" />


 <img width="1186" height="620" alt="image" src="https://github.com/user-attachments/assets/2b3b37b8-27f8-4875-95e5-17f910c8457c" />
**ProTeGi** is about improving **prompt engineering** using **textual (or natural language) gradients**.

Let‚Äôs unpack it *step by step* so it‚Äôs clear.

---

### üß© 1. Natural Language Gradients

Think of these like **gradients in regular machine learning**, but instead of numbers, they happen in **language space**.

* In normal ML, a *gradient* tells you which direction to move to improve a model (e.g., lower loss).
* Here, a *natural language gradient* tells you how to improve a **prompt** ‚Äî it points out *what‚Äôs wrong* with the current text.

üëâ Example:
If your prompt is ‚ÄúSummarize this text briefly,‚Äù but the model keeps giving long answers,
the *natural language gradient* might say:

> ‚ÄúThe prompt should emphasize conciseness more strongly.‚Äù

So it‚Äôs like feedback ‚Äî the LLM critiques your prompt.

---

### ‚úèÔ∏è 2. Prompt Editing

Once you have those ‚Äúgradients‚Äù (textual feedback), you edit the prompt to fix the issues.

The model uses another instruction to rewrite the prompt *in the opposite direction* of the problems.
In our example, it might change the prompt to:

> ‚ÄúSummarize this text in **one or two short sentences**.‚Äù

That‚Äôs moving ‚Äúsemantically opposite‚Äù to the problem the gradient described.

---

### üîç 3. Beam Search and Bandit Selection

Now, when improving prompts, there are many possible edits ‚Äî too many to test them all.
So these algorithms help you **search efficiently** for the best one:

* **Beam search** ‚Üí keeps a small set of the best prompt candidates as it explores.
* **Bandit selection** ‚Üí tries different options and uses feedback (like success rates) to pick the best-performing prompts efficiently.

So together, they help the system *explore many prompt variations without wasting compute.*

---
Beam Search and Bandit Selection is a key part of how the system *chooses* which prompts to keep.

Let‚Äôs unpack both ideas simply, then connect them to prompt optimization.

---

### üéØ 1. UCB Bandits (Upper Confidence Bound)

This comes from the **multi-armed bandit problem** ‚Äî imagine you‚Äôre at a casino with many slot machines.
Each machine gives random rewards, and you want to find the best one without wasting time.

The UCB strategy says:

> ‚ÄúTry each machine a bit, then keep choosing the one with the **best average reward + some uncertainty margin**.‚Äù

That ‚Äúmargin‚Äù (the **upper confidence bound**) makes you occasionally explore machines you‚Äôre less sure about ‚Äî so you don‚Äôt get stuck on a ‚Äúpretty good‚Äù one too early.

üß† In prompt tuning:
Each **prompt candidate** is like a slot machine.
The ‚Äúreward‚Äù is how well the model performs (say, accuracy or score).
UCB helps you test a few prompts and then focus on the most promising ones.

---

### üîÅ 2. Successive Rejects

This is another bandit strategy, but simpler.
Instead of adding uncertainty bonuses, it works in **rounds**:

1. Test all prompts for a while.
2. Eliminate the **worst-performing** one.
3. Test the rest more deeply.
4. Repeat until one best prompt remains.

So it‚Äôs a **gradual elimination** method.

üß† In prompt tuning:
You start with many edited prompts ‚Üí evaluate ‚Üí drop the weakest ‚Üí keep refining the survivors.

---

### üîç Summary:

| Method                 | Core Idea                                            | How It Helps Prompt Search          |
| ---------------------- | ---------------------------------------------------- | ----------------------------------- |
| **UCB Bandits**        | Balances trying new options vs. exploiting good ones | Avoids missing better prompts       |
| **Successive Rejects** | Eliminates the worst options step by step            | Efficiently narrows down candidates |


---

<img width="956" height="488" alt="image" src="https://github.com/user-attachments/assets/58196b03-6f14-43f7-9881-e7667c3e8b28" />


### üß† What ProTeGi does (in plain terms)

Think of ProTeGi as a **smart system for improving prompts automatically**.

It does two big things at once:

1. **Learning from mistakes** ‚Üí ‚ÄúNatural Language Gradients‚Äù

   * It looks at how a prompt performs.
   * The model gives *textual feedback* (like, ‚ÄúThis prompt is too vague‚Äù).
   * That feedback acts like a *gradient* ‚Äî it points out which direction to improve.

2. **Strategic exploration of new prompts** ‚Üí ‚ÄúBeam Search + Bandit Selection‚Äù

   * Instead of randomly guessing new prompts, it tries several candidates.
   * It then **selects the most promising ones** using smart search (beam search) and efficient testing (UCB or Successive Rejects).

Together, that means ProTeGi is learning and exploring **like an optimizer**, but using *language* instead of math equations.

---

### üéØ Goal

> ‚ÄúFind the optimal configuration in fewer steps.‚Äù

That means: reach the best-performing prompt as quickly and efficiently as possible ‚Äî fewer experiments, better results.

---

### ‚ö†Ô∏è Limitation

> ‚ÄúRequires numerous API calls, which can be computationally expensive.‚Äù

Each time the system:

* tests a prompt,
* gets feedback,
* edits it, and
* re-evaluates,

it needs to query an LLM again ‚Äî so the process can be **costly and slow** if not managed carefully.



-----


Let‚Äôs unpack that phrase: **‚Äúmoves semantically opposite to the gradients.‚Äù**

---

### 1Ô∏è‚É£ First ‚Äî what does a *gradient* mean here?

In normal machine learning, a **gradient** tells you the *direction of greatest error*.
So, if you follow the gradient, you move **toward** more error.
If you move **opposite to it**, you reduce error ‚Äî you‚Äôre *improving*.

---

### 2Ô∏è‚É£ In **natural language gradients**:

The ‚Äúgradient‚Äù is **textual feedback** describing *what‚Äôs wrong* with the current prompt.

Example:

> Gradient (feedback): ‚ÄúThe prompt is too broad; it needs more specific instructions.‚Äù

This is like a ‚Äúsemantic direction‚Äù ‚Äî it points toward the *problem*.

---

### 3Ô∏è‚É£ Moving **semantically opposite** means:

You adjust the prompt in the **opposite direction** of that problem.

So instead of being broad, you make the prompt **more specific**.
Instead of being vague, you make it **clearer**.
Instead of missing constraints, you **add them**.

---

### üí° In short:

> **Natural language gradient** = describes what‚Äôs wrong (the direction of weakness).
> **Semantic movement opposite to it** = editing the prompt to fix that weakness.

---
<img width="1183" height="703" alt="image" src="https://github.com/user-attachments/assets/e1de953e-a384-4980-b7a7-b62453cbd2fb" />


# Repo
- https://github.com/zou-group/textgrad

<img width="3725" height="1030" alt="image" src="https://github.com/user-attachments/assets/6ff0d694-61a1-494a-8060-2590ca31eedb" />

TextGrad: Automatic ''Differentiation'' via Text

An autograd engine -- for textual gradients!

TextGrad is a powerful framework building automatic ``differentiation'' via text. TextGrad implements backpropagation through text feedback provided by LLMs, strongly building on the gradient metaphor

We provide a simple and intuitive API that allows you to define your own loss functions and optimize them using text feedback. This API is similar to the Pytorch API, making it simple to adapt to your usecases.


- code analysis:

  - https://zread.ai/zou-group/textgrad/9-automatic-differentiation-for-text
  
  TextGrad revolutionizes the way we optimize text-based systems by implementing automatic "differentiation" through text feedback. This powerful framework extends the familiar concept of gradient-based optimization from numerical domains to the world of text, enabling us to optimize prompts, solutions, and other text-based variables using large language models (LLMs) as gradient engines.

  ## The Gradient Metaphor in Text Space
  In traditional deep learning, gradients are numerical values that indicate how to adjust parameters to minimize a loss function. TextGrad extends this metaphor to text by treating textual feedback as gradients. Instead of calculating derivatives mathematically, TextGrad uses LLMs to generate descriptive feedback that guides how text variables should be improved.
  
  TextGrad doesn't compute numerical derivatives - it generates textual gradients that explain how and why a text variable should be changed to improve performance.
    
