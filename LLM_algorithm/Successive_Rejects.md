
# From AI:

### ğŸ¯ 1. UCB Bandits (Upper Confidence Bound)

This comes from the **multi-armed bandit problem** â€” imagine youâ€™re at a casino with many slot machines.
Each machine gives random rewards, and you want to find the best one without wasting time.

The UCB strategy says:

> â€œTry each machine a bit, then keep choosing the one with the **best average reward + some uncertainty margin**.â€

That â€œmarginâ€ (the **upper confidence bound**) makes you occasionally explore machines youâ€™re less sure about â€” so you donâ€™t get stuck on a â€œpretty goodâ€ one too early.

ğŸ§  In prompt tuning:
Each **prompt candidate** is like a slot machine.
The â€œrewardâ€ is how well the model performs (say, accuracy or score).
UCB helps you test a few prompts and then focus on the most promising ones.

---

### ğŸ” 2. Successive Rejects

This is another bandit strategy, but simpler.
Instead of adding uncertainty bonuses, it works in **rounds**:

1. Test all prompts for a while.
2. Eliminate the **worst-performing** one.
3. Test the rest more deeply.
4. Repeat until one best prompt remains.

So itâ€™s a **gradual elimination** method.

ğŸ§  In prompt tuning:
You start with many edited prompts â†’ evaluate â†’ drop the weakest â†’ keep refining the survivors.

---

### ğŸ” Summary:

| Method                 | Core Idea                                            | How It Helps Prompt Search          |
| ---------------------- | ---------------------------------------------------- | ----------------------------------- |
| **UCB Bandits**        | Balances trying new options vs. exploiting good ones | Avoids missing better prompts       |
| **Successive Rejects** | Eliminates the worst options step by step            | Efficiently narrows down candidates |

---


Letâ€™s break down **Successive Rejects** step by step and connect it to prompt tuning.

---

### ğŸ§© The idea:

Successive Rejects is another way to solve the **multi-armed bandit** problem â€” finding the best option (like the best prompt) by testing each one multiple times.

Instead of balancing exploration like UCB does, it uses a **tournament-style elimination**:

1. **Start with all candidates.**
   Say you have 5 prompt versions.
2. **Test each one** a few times and record their average rewards.
3. **Reject (eliminate) the worst one.**
4. **Repeat the process** with the remaining candidates.
5. Continue until only one â€œwinnerâ€ remains â€” thatâ€™s your best prompt.

---

### ğŸ¯ Why itâ€™s called *Successive Rejects*:

Because in each round, you **successively reject** the least promising option.

Itâ€™s a bit like:

> â€œWeâ€™ll give everyone a fair chance early on, but weâ€™ll gradually spend more time testing the stronger ones.â€

---

### ğŸ’¡ Example (prompt optimization version)

Imagine youâ€™re testing 4 prompts:

| Round | Prompts Tested | Eliminate         | Remaining |
| ----- | -------------- | ----------------- | --------- |
| 1     | A, B, C, D     | D (worst)         | A, B, C   |
| 2     | A, B, C        | C (next worst)    | A, B      |
| 3     | A, B           | compare carefully | A wins    |

By testing and rejecting in stages, you **use your testing budget efficiently** â€” fewer tests wasted on poor candidates.

---

### âš™ï¸ In ProTeGi:

Successive Rejects helps the system:

* Focus on the most promising **prompt edits**.
* Drop weak prompts early.
* End up with the strongest-performing version using fewer evaluations.

---


